# -*- coding: utf-8 -*-
"""Churn using ANN .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkb3SxRABUncm-tTpIfVpKNErne4LUsv
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
# %matplotlib inline
import seaborn as sns

path='/content/WA_Fn-UseC_-Telco-Customer-Churn.csv'
df= pd.read_csv(path)

pd.options.display.max_columns =None
df.head(5)

df.shape

df.drop('customerID',axis = 1,inplace=True)

df.head(10)

df.dtypes

df.TotalCharges.values

df.MonthlyCharges.values

df["TotalCharges"] = pd.to_numeric(df["TotalCharges"],errors='coerce')

df.info()

df.info

df.head(5)

df.isnull().sum()

df= df.dropna()

df.isnull().sum()

df.describe()

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
df['TotalCharges']=sc.fit_transform(pd.DataFrame(df['TotalCharges']))#feature scaling

df.describe()

sns.pairplot(df)

box_data = df 
box_target = df.MonthlyCharges 
sns.boxplot(data = box_data,width=0.5,fliersize=5)
sns.set(rc={'figure.figsize':(2,15)})

sns.countplot(df['Churn'])#highly imabalnced

"""**Undersampling, Oversampling, Underfitting, Overfitting**

Within statistics, Oversampling and undersampling in data analysis are

techniques used to adjust the class distribution of a data set (i.e. the ratio between the different classes/categories represented). These terms are used both in statistical sampling, survey design methodology and in machine learning.

# **[Undersampling]**
"""

Not_Churn = df[df['Churn']=='No']
Churned = df[df['Churn']=='Yes']

Not_Churn.shape

Churned.shape

Not_Churn_Sample =Not_Churn.sample(n=1869)# randomly took 1869 rows

Not_Churn_Sample.shape

new_df = pd.concat([Not_Churn_Sample,Churned],ignore_index=True)

new_df['Churn'].value_counts()

new_df.head(5)

OHE_feature = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling','gender','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']

new_df = pd.get_dummies(new_df, columns=OHE_feature)

pd.set_option('max_columns', None)

new_df.head()

new_df.info()

new_df['Churn'].nunique()
new_df['Churn'].unique()

new_df.loc[new_df['Churn']=='No','Churn'] = 0
new_df.loc[new_df['Churn']=='Yes','Churn'] = 1

new_df.head(5)

"""Changing it 0 and 1 as logistic regression takes 0 and 1"""

new_df.isnull().sum()

new_df["Churn"] = pd.to_numeric(new_df["Churn"],errors='coerce')

new_df.info()

plt.figure(figsize = (50,50))
corr_matrix = new_df.corr() # Corelation Matrix
sns.heatmap(corr_matrix,cmap ="mako",annot=True,center=0,vmin=0.1,vmax=0.9,cbar = False)
sns.set(font_scale=1.5)
plt.show()

X= new_df.drop('Churn',axis=1)#independent variables or features are in X
y= new_df['Churn']#Target Variable

y.shape

X.shape

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.25,random_state=1)

"""People having contract month to month and dont use tech support are mostly corelated with churn yes"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score,recall_score,f1_score

"""**Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=1)
clf =  DecisionTreeClassifier().fit(X_train, y_train)
predicted = clf.predict(X_test)

predicted = clf.predict(X_test)

DC_scr=accuracy_score(y_test, predicted)
DC_scr

precision_score(y_test,predicted)

recall_score(y_test,predicted)

"""**KNN**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
KNN_scr =accuracy_score(y_test, pred)

KNN_scr

precision_score(y_test,pred)

recall_score(y_test,pred)

"""**Logistic regression**"""

from sklearn.linear_model import LogisticRegression

log = LogisticRegression()
log.fit(X_train,y_train)
y_pred1=log.predict(X_test)
accuracy_score(y_test,y_pred1)

precision_score(y_test,y_pred1)

recall_score(y_test,y_pred1)

"""#Lets try **Oversampling**"""

df.head(5)

OHE_feature = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling','gender','MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']

df = pd.get_dummies(df, columns=OHE_feature)

df.loc[df['Churn']=='No','Churn'] = 0
df.loc[df['Churn']=='Yes','Churn'] = 1

df.head()

X_new= df.drop('Churn',axis=1)#independent variables or features are in X
y_new= df['Churn']#Target Variable

X_new.shape

y_new.shape

y_new.nunique()

y_new.unique()

df['Churn'].value_counts()

df.info()

df["Churn"] = pd.to_numeric(df["Churn"],errors='coerce')

df.isnull().sum()

df.info()

y_new=y_new.astype('int')

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=1)
X_res,y_res =sm.fit_resample(X_new,y_new)

X_res.shape

y_res.shape

y_res.value_counts()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X_res,y_res,test_size=0.25,random_state=30)

y_test.shape

_log=LogisticRegression()
log.fit(X_train,y_train)
y_pred2 = log.predict(X_test)
accuracy_score(y_test,y_pred2)

print(accuracy_score(y_test,y_pred2),precision_score(y_test,y_pred2),recall_score(y_test,y_pred2),f1_score(y_test,y_pred2))

"""**Decision Tree Classifier**"""

dt =DecisionTreeClassifier()
dt.fit(X_train,y_train)
y_pred3 = dt.predict(X_test)

print(accuracy_score(y_test,y_pred2),precision_score(y_test,y_pred3),recall_score(y_test,y_pred3),f1_score(y_test,y_pred3))

"""**KNN**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train,y_train)
pred5 = knn.predict(X_test)
KNN_scr =accuracy_score(y_test, pred5)

print(accuracy_score(y_test,pred5),precision_score(y_test,pred5),recall_score(y_test,pred5),f1_score(y_test,pred5))

"""# Neural Network"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X_res,y_res,test_size=0.25,random_state=30)

X_train.shape

X_test.shape

import tensorflow as tf
from tensorflow import keras
from keras.layers.core import Dropout

model = keras.Sequential([
    keras.layers.Dense(100, input_shape=(45,), activation ='relu',Dropout =(0.2)),
    keras.layers.Dense(60, activation ='relu'),
    keras.layers.Dense(30, activation ='relu'),
    keras.layers.Dense(1,activation ='sigmoid'),

])

model.compile(optimizer ='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100)

model.evaluate(X_test,y_test)

ypred= model.predict(X_test)
ypred[:10]

y_test[:10]

y_pred =[]
for element in ypred:
  if element >0.5:
    y_pred.append(1)
  else:
    y_pred.append(0)

y_pred[:10]

from sklearn.metrics import confusion_matrix , classification_report
print(classification_report(y_test,y_pred))

cm=tf.math.confusion_matrix(labels=y_test,predictions = y_pred)

plt.figure(figsize=(5,5))
sns.heatmap(cm,annot =True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

round((1067+1119)/(1067+207+189+1119),2)

"""# **NN second attempt without importing **




"""

#X_res.shape

#y_res.shape

#def sigmoid(z):
 # return 1 / (1 + np.exp(-z) )

#input_layer_size = 45           
#hidden_layer_size = 25
#output_layer_size = 1

"""Where is This theta 1 and theta 2 in my dataset"""

# a1: activation (input) of 1st layer

# z2: hypothesis of 1st layer
# a2: sigmoid of z2. Output of 1st layer and activation (input) of 2nd layer
# ...


def costFunction(parameters, X_res, y_res, Lambda=0.1):
  J = 0
  m = len(X_res)

  # Compute output of hidden layer
  ones = np.ones((len(X_res), 1))                            
  a1 = np.append(ones, X_res, axis=1)         # add Bias
  z2 = np.dot(theta1, a1.T)                           
  a2 = sigmoid(z2)      

  # Compute output probabilities
  ones = np.ones((1, a2.shape[1]))                  
  a2 = np.append(ones, a2, axis=0)        # Add Bias      
  z3 = np.dot(theta2, a2)                      
  a3 = sigmoid(z3)                        # a3 is our last, i.e.output layer

  predictions = a3                        # actually, it's vector of predicted propabilities

  for hx_i, y in zip(predictions.T, Y):
    y_i = np.zeros((output_layer_size))   # Vector of answers [10 x 1]
    idx = y-1                             # Index of "correct" label
    y_i[idx] = 1                          # Set 1 on correct index, e.g.: [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]

    J += np.dot(-y_i, np.log(hx_i)) - np.dot( (1-y_i), np.log(1-hx_i) )

  # Add regularization term
  reg = Lambda/(2*m) * (np.sum(theta1[:,1:]**2) + np.sum(theta2[:,1:]**2))    # Do not include bias column into regularization
  J = 1/m * J + reg

  ## Gradients

  DELTA1 = np.zeros(theta1.shape)
  DELTA2 = np.zeros(theta2.shape)

  for x, y in zip(X, Y):               
      # Activation of 1st hidden layer
      a1 = np.insert(x, 0, [1])                   # add bias
      z2 = np.dot(theta1, a1.T) 
      a2 = sigmoid(z2)

      # Activation of output layer
      a2 = np.insert(a2, 0, [1])                  # add bias
      z3 = np.dot(theta2, a2)
      a3 = sigmoid(z3)

      # Delta of output layer
      y_i = np.zeros(output_layer_size)          # Vector of true value for given sample
      y_i[y-1] = 1                               # Set 1 at correct index, e.g.: [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]

      # Delta of output layer
      delta3 = a3 - y_i

      # Delta of hiden layer
      gz2 = a2 * (1-a2)                          # [26 x 1] .* [26 x 1]
      delta2 = np.dot(theta2.T, delta3) * gz2    # [26 x 10] * [10 x 1] .* [26 x 1] = [26 x 1]
      
      DELTA1 = DELTA1 + np.dot(delta2[1:].reshape(len(delta2[1:]), 1), a1.reshape((len(a1), 1)).T)
      DELTA2 = DELTA2 + np.dot(delta3.reshape((len(delta3), 1)), a2.reshape((len(a2), 1)).T)
      
  grad1 = DELTA1 / m
  grad2 = DELTA2 / m

  # Regularizatin
  grad1 = grad1 + (Lambda/m) * np.hstack((np.zeros((theta1.shape[0],1)), theta1[:,1:]))     # Regularized gradient 1 (without bias)
  grad2 = grad2 + (Lambda/m) * np.hstack((np.zeros((theta2.shape[0],1)), theta2[:,1:]))     # Regularized gradient 2 (without bias)
  
  return J, grad1, grad2


J, _, _ = costFunction(parameters, X, Y, Lambda=0)
J

"""# Logistic Regression by formula using oversampled results.(X_res, y_res)"""

df_lr = pd.concat([X_res,y_res],ignore_index=True)

df_lr.head(5)

df_lr.info()

df_lr[0].value_counts() # its actually churn

"""rename it if you want"""

import math

math.e # hint: euler's number

def sigmoid(z):
  # Try to implement on your own
  return 1 / (1 + math.e ** -z )

X_res.shape

y_res.shape

m = len(y_res)                      
X = X_res.to_numpy()                
Y = y_res.to_numpy()
X

np.transpose(X) == X.T

number_of_features = X_res.shape[1] +1
initial_theta = np.zeros((number_of_features, 1))

number_of_features = X.shape[1] +1
initial_theta = np.zeros((number_of_features, 1)) # Why [3 x 1] when we have only 2 features?

def costFunction(theta, X, Y):
  ### Add new column of ones into X (to allow calculate dot product)
  ones = np.ones((len(X), 1))
  X_ones = np.append(ones, X, axis=1)

  grad = np.zeros((len(X), 1))

  argument = np.dot(theta.T, X_ones.T)    # [1x3] * [3x100] = [1x100]

  h_x = sigmoid(argument)

  # J = (-1/m) * ( np.dot( Y.T, np.log(h_x.T) ) + np.dot( (1-Y).T, np.log(1-h_x.T) ) )  # [1x100] * [100x1] + [1x100] * [100x1]
  ### Or alternatively
  A = np.dot( Y.T, np.log(h_x.T) )                  # [1x100] * [100x1]
  B = np.dot( (1-Y).T, np.log(1-h_x.T) )            # [1x100] * [100x1]
  J = (-1/m) * (A+B)                                # [1]
  return J


costFunction(theta=initial_theta, X=X, Y=Y)

